Excellent choice ðŸ‘
This is the **right** thing to do *before* agents. Async + cancellation is what makes AI autocomplete feel *native* instead of â€œlaggyâ€.

Iâ€™ll be very explicit and very pragmatic here, because pygls async + LSP cancellation has a few sharp edges.

---

# Phase 4.5 â€“ Async completions + cancellation (pygls v2)

## Goal (definition of done)

By the end of this phase:

* Completion handler is **async**
* Ollama calls do **not block** the LSP thread
* Old requests are **cancelled** when the user keeps typing
* Neovim stays responsive even with slow LLMs

---

## 1. Reality check: how cancellation works in LSP

Important mental model:

* The client sends:

  * `textDocument/completion`
* If the user types again, the client may send:

  * `$/cancelRequest` with the *same request id*

ðŸ‘‰ **Cancellation is cooperative**

* LSP server must **notice** it
* Python cannot kill a thread mid-call
* We must design for *best-effort cancellation*

---

## 2. pygls v2 async fundamentals (important)

In pygls v2:

* Handlers **can be `async def`**
* pygls runs them in an asyncio event loop
* Blocking I/O must be moved off the loop

ðŸš¨ `requests` is blocking
So we must:

* either move it to a thread
* or switch to `aiohttp`

Weâ€™ll do it **cleanly** with `asyncio.to_thread()` first.

---

## 3. Make the engine async-aware (small refactor)

### Update the engine interface

```python
# ai_lsp/ai/engine.py

from abc import ABC, abstractmethod
from ai_lsp.domain.completion import CompletionContext


class CompletionEngine(ABC):
    @abstractmethod
    async def complete(self, context: CompletionContext) -> str:
        """Return a code completion string"""
        raise NotImplementedError
```

Yes â€” async at the boundary.

---

## 4. Make Ollama engine async-safe (threaded)

### Update `ollama_client.py`

```python
# ai_lsp/ai/ollama_client.py

import asyncio
import requests
from ai_lsp.ai.engine import CompletionEngine
from ai_lsp.domain.completion import CompletionContext


class OllamaCompletionEngine(CompletionEngine):
    def __init__(
        self,
        model: str = "codellama:7b",
        base_url: str = "http://localhost:11434",
        timeout: int = 10,
    ):
        self.model = model
        self.base_url = base_url
        self.timeout = timeout

    async def complete(self, context: CompletionContext) -> str:
        return await asyncio.to_thread(self._blocking_complete, context)

    def _blocking_complete(self, context: CompletionContext) -> str:
        prompt = self._build_prompt(context)

        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.2,
                    "num_predict": 64,
                },
            },
            timeout=self.timeout,
        )

        response.raise_for_status()
        return response.json().get("response", "").strip()

    def _build_prompt(self, context: CompletionContext) -> str:
        previous = "\n".join(context.previous_lines)

        return f"""
You are a code autocomplete engine.
Return ONLY the completion text.

Language: {context.language}
Context:
{previous}

Current line:
{context.current_line}

Prefix:
{context.prefix}

Completion:
""".strip()
```

### Why this is correct *for now*

* No event-loop blocking
* No HTTP client rewrite yet
* Easy to swap later for `aiohttp`

---

## 5. Async completion handler

Now the LSP side.

### Update completion handler to `async def`

```python
@server.feature(types.TEXT_DOCUMENT_COMPLETION)
async def on_completion(ls, params):
```

---

## 6. Add cancellation tracking (core of this phase)

We need:

* One active completion task per document
* Cancel previous task when a new one arrives

### Add this near `register_completion`

```python
import asyncio
from typing import Dict
```

Inside `register_completion`:

```python
active_tasks: Dict[str, asyncio.Task] = {}
```

Keyed by document URI.

---

## 7. Full async + cancellation handler

Here is the **important part**.

```python
@server.feature(types.TEXT_DOCUMENT_COMPLETION)
async def on_completion(ls, params):
    uri = params.text_document.uri
    document = documents.get(uri)

    if not document:
        return CompletionList(is_incomplete=True, items=[])

    context = context_builder.build(document, params.position)

    # Guard: avoid LLM spam
    if len(context.prefix.strip()) < 2:
        return CompletionList(is_incomplete=True, items=[])

    # Cancel previous task for this document
    previous_task = active_tasks.get(uri)
    if previous_task and not previous_task.done():
        previous_task.cancel()

    async def run_completion():
        return await engine.complete(context)

    task = asyncio.create_task(run_completion())
    active_tasks[uri] = task

    try:
        completion = await task
    except asyncio.CancelledError:
        return CompletionList(is_incomplete=True, items=[])
    except Exception as e:
        ls.show_message_log(f"Ollama error: {e}")
        return CompletionList(is_incomplete=True, items=[])

    if not completion:
        return CompletionList(is_incomplete=True, items=[])

    item = CompletionItem(
        label=completion.splitlines()[0][:80],
        insert_text=completion,
        kind=CompletionItemKind.Text,
    )

    return CompletionList(is_incomplete=False, items=[item])
```

---

## 8. What cancellation now means (be precise)

When you type quickly:

1. Neovim sends completion request A
2. You type again
3. Neovim sends request B
4. Server:

   * cancels task A
   * starts task B
5. Task A:

   * may still be running in background thread
   * but its result is **ignored**
   * UI stays responsive

This is **exactly how most AI tools behave**.

---

## 9. What we intentionally did NOT do (yet)

* âŒ true HTTP cancellation (requests canâ€™t)
* âŒ streaming tokens
* âŒ multi-result ranking
* âŒ speculative prefetch

Those come later.

---

## 10. Smoke test (very important)

### Test behavior

* Enable autocomplete on every keystroke
* Type quickly:

  ```python
  user.name.fo
  ```
* You should see:

  * no freezing
  * old suggestions disappearing
  * only latest completion appearing

If Neovim stays smooth â†’ **Phase 4.5 complete** âœ…

---

## 11. Why this unlocks everything else

Now you can safely add:

* agents
* intent detection
* retries
* caching
* streaming

Without rewriting the LSP layer again.

---

## Next step (natural progression)

Youâ€™re now at a fork:

### â–¶ Phase 5 â€“ Completion Agents

Add:

* intent agent (cheap)
* context pruning
* LLM arbitration

OR

### â–¶ Phase 6 â€“ Streaming ghost text (inline completions)

This is UX magic, but more Neovim-specific.

Say the word:

* **â€œLetâ€™s do Phase 5 (agents)â€**
* or **â€œLetâ€™s do streaming inline completionsâ€**

Youâ€™ve built a *real* AI LSP at this point.

