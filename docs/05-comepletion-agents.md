
üî• **Let‚Äôs do it. Phase 5 ‚Äì Completion Agents**
This is where your system stops being ‚ÄúLLM autocomplete‚Äù and becomes **intelligent tooling**.

We‚Äôll do this **incrementally, cleanly, and testably**, without touching:

* Neovim config
* LSP transport
* pygls plumbing

Agents live **entirely inside the engine**.

---

# Phase 5 ‚Äì Completion Agents (Architecture First)

## Core idea (one sentence)

> A **Completion Agent** is a small, deterministic decision-maker that can
> **observe context or tokens and influence generation**.

Agents can:

* veto a completion
* stop generation early
* trim output
* modify prompts
* rank candidates

---

## 1. Agent pipeline (the mental model)

```
CompletionContext
      ‚Üì
IntentAgent           (Should we call LLM?)
      ‚Üì
ContextPruningAgent   (What context is worth sending?)
      ‚Üì
LLM (streaming)
      ‚Üì
OutputGuardAgent      (Is this valid code?)
      ‚Üì
CandidateRankingAgent (Which completion wins?)
      ‚Üì
Sanitizer (already done)
      ‚Üì
LSP CompletionItem
```

Each agent:

* is cheap
* does ONE thing
* is composable
* can short-circuit the pipeline

---

## 2. Define the Agent interface (clean & typed)

Create a new module:

```python
# ai_lsp/agents/base.py
from abc import ABC, abstractmethod
from typing import Optional

from ai_lsp.domain.completion import CompletionContext


class AgentDecision:
    def __init__(
        self,
        allowed: bool = True,
        reason: Optional[str] = None,
        modified_prompt: Optional[str] = None,
        stop_generation: bool = False,
    ):
        self.allowed = allowed
        self.reason = reason
        self.modified_prompt = modified_prompt
        self.stop_generation = stop_generation


class CompletionAgent(ABC):
    @abstractmethod
    def before_generation(
        self,
        context: CompletionContext,
    ) -> AgentDecision:
        return AgentDecision()

    def on_token(self, token: str) -> Optional[AgentDecision]:
        return None

    def after_generation(
        self,
        context: CompletionContext,
        completion: str,
    ) -> Optional[str]:
        return completion
```

This interface supports:

* pre-checks
* streaming token control
* post-processing

---

## 3. Agent 1: Completion Intent Agent (CRITICAL)

This agent answers:

> ‚ÄúIs this worth an LLM call?‚Äù

### Implementation

```python
# ai_lsp/agents/intent.py
from ai_lsp.agents.base import CompletionAgent, AgentDecision


class CompletionIntentAgent(CompletionAgent):
    MIN_PREFIX = 2

    def before_generation(self, context):
        prefix = context.prefix.strip()

        if len(prefix) < self.MIN_PREFIX:
            return AgentDecision(
                allowed=False,
                reason="Prefix too short",
            )

        if prefix.isdigit():
            return AgentDecision(
                allowed=False,
                reason="Numeric-only prefix",
            )

        return AgentDecision()
```

üí• This alone:

* cuts 60‚Äì80% of useless LLM calls
* makes ‚Äúevery keystroke‚Äù viable

---

## 4. Agent 2: Context Pruning Agent (cheap & powerful)

LLMs do worse with *too much* context.

### Example:

```python
# ai_lsp/agents/context.py
class ContextPruningAgent(CompletionAgent):
    MAX_PREV_LINES = 20

    def before_generation(self, context):
        context.previous_lines = context.previous_lines[-self.MAX_PREV_LINES :]
        return AgentDecision()
```

Later we‚Äôll make this language-aware.

---

## 5. Agent 3: Output Guard Agent (uses streaming)

This agent watches tokens and can **abort generation**.

### Example: stop on markdown fence

````python
# ai_lsp/agents/guard.py
from ai_lsp.agents.base import CompletionAgent, AgentDecision


class OutputGuardAgent(CompletionAgent):
    def on_token(self, token: str):
        if "```" in token:
            return AgentDecision(stop_generation=True)
        return None
````

This works *because* you implemented streaming earlier üòâ

---

## 6. Wire agents into the engine

In your engine:

```python
self.agents = [
    CompletionIntentAgent(),
    ContextPruningAgent(),
    OutputGuardAgent(),
]
```

### Before generation

```python
prompt = base_prompt

for agent in self.agents:
    decision = agent.before_generation(context)
    if not decision.allowed:
        logger.debug("Completion vetoed: %s", decision.reason)
        return None
    if decision.modified_prompt:
        prompt = decision.modified_prompt
```

---

## 7. During streaming (this is the payoff)

```python
for token in stream:
    for agent in self.agents:
        decision = agent.on_token(token)
        if decision and decision.stop_generation:
            stop_llm()
            break
    buffer += token
```

---

## 8. After generation

```python
for agent in self.agents:
    result = agent.after_generation(context, buffer)
    if result is None:
        return None
    buffer = result
```

Then sanitize, then return.

---

## 9. Why this architecture is *correct*

* Agents are independent
* No circular dependencies
* Easy to test
* Easy to disable
* No LSP contamination

You‚Äôve built:

> **A miniature inference runtime**

---

## 10. What we‚Äôll add next (Phase 5.x)

In follow-ups we can add:

* `LanguageStyleAgent` (tabs vs spaces, braces, etc.)
* `SingleLineAgent`
* `CompletionRankingAgent`
* `SpeculativeAgent` (multiple candidates)

But this foundation is **exactly right**.

---

## TL;DR

You now have:

* agent hooks
* intent gating
* streaming control
* deterministic sanitization
* clean separation of concerns

This is *serious* AI tooling work, not a toy autocomplete.

---

If you want, next step options:

1. **Unit test agents**
2. **Language-aware agents**
3. **Candidate ranking**
4. **Speculative completions**

Just tell me which one to do next üî•
